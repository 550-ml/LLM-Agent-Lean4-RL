# LLM æ¨¡å—æ”¹è¿›è¯´æ˜

## âœ… å·²å®Œæˆçš„æ”¹è¿›

### 1. **å¢å¼ºçš„é…ç½®ç±» (LLMConfig)**

æ–°å¢é…ç½®é¡¹ï¼š
- `frequency_penalty`: é¢‘ç‡æƒ©ç½šï¼ˆ0.0-2.0ï¼‰
- `presence_penalty`: å­˜åœ¨æƒ©ç½šï¼ˆ0.0-2.0ï¼‰
- `max_retries`: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 3ï¼‰
- `retry_delay`: é‡è¯•å»¶è¿Ÿï¼ˆé»˜è®¤ 1.0 ç§’ï¼‰

### 2. **é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶**

- âœ… **è‡ªåŠ¨é‡è¯•**: é‡åˆ° RateLimitError æˆ– APIConnectionError æ—¶è‡ªåŠ¨é‡è¯•
- âœ… **æŒ‡æ•°é€€é¿**: é‡è¯•å»¶è¿ŸæŒ‰æŒ‡æ•°å¢é•¿ï¼ˆ1s, 2s, 4s...ï¼‰
- âœ… **é”™è¯¯åˆ†ç±»**: åŒºåˆ†ä¸åŒç±»å‹çš„é”™è¯¯ï¼ˆé€Ÿç‡é™åˆ¶ã€è¿æ¥é”™è¯¯ã€API é”™è¯¯ï¼‰
- âœ… **è¯¦ç»†æ—¥å¿—**: è®°å½•æ¯æ¬¡é‡è¯•å’Œé”™è¯¯ä¿¡æ¯

### 3. **æ¶ˆæ¯éªŒè¯**

- âœ… **æ ¼å¼æ£€æŸ¥**: éªŒè¯æ¶ˆæ¯æ ¼å¼æ˜¯å¦æ­£ç¡®
- âœ… **è§’è‰²éªŒè¯**: ç¡®ä¿è§’è‰²æ˜¯ system/user/assistant ä¹‹ä¸€
- âœ… **å†…å®¹æ£€æŸ¥**: ç¡®ä¿æ¯æ¡æ¶ˆæ¯éƒ½æœ‰ content å­—æ®µ

### 4. **Token è®¡æ•°æ”¹è¿›**

- âœ… **tiktoken æ”¯æŒ**: ä½¿ç”¨ tiktoken ç²¾ç¡®è®¡ç®— token
- âœ… **o1 ç³»åˆ—æ”¯æŒ**: æ­£ç¡®å¤„ç† o1-preview å’Œ o1-mini çš„ç¼–ç 
- âœ… **é™çº§å¤„ç†**: å¦‚æœ tiktoken ä¸å¯ç”¨ï¼Œä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—

### 5. **æ¨¡å‹ç‰¹æ®Šå¤„ç†**

- âœ… **o1 ç³»åˆ—**: è‡ªåŠ¨è·³è¿‡ä¸æ”¯æŒçš„å‚æ•°ï¼ˆfrequency_penalty, presence_penaltyï¼‰
- âœ… **å‚æ•°é€‚é…**: æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨è°ƒæ•´ API å‚æ•°

### 6. **å·¥å…·å‡½æ•° (utils.py)**

æ–°å¢å®ç”¨å‡½æ•°ï¼š
- `estimate_cost()`: ä¼°ç®— API è°ƒç”¨æˆæœ¬
- `format_messages_for_logging()`: æ ¼å¼åŒ–æ¶ˆæ¯ç”¨äºæ—¥å¿—
- `validate_config()`: éªŒè¯é…ç½®æœ‰æ•ˆæ€§

### 7. **æ—¥å¿—è®°å½•**

- âœ… **è°ƒè¯•æ—¥å¿—**: è®°å½• API è°ƒç”¨è¯¦æƒ…
- âœ… **è­¦å‘Šæ—¥å¿—**: è®°å½•é‡è¯•å’Œé™çº§æ“ä½œ
- âœ… **é”™è¯¯æ—¥å¿—**: è®°å½•æ‰€æœ‰é”™è¯¯ä¿¡æ¯

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
from src.llm.factory import LLMFactory
from src.llm.base import LLMConfig

# åˆ›å»ºé…ç½®
config = LLMConfig(
    model_name="gpt-4o",
    temperature=0.7,
    max_tokens=2048,
    max_retries=3  # è‡ªåŠ¨é‡è¯• 3 æ¬¡
)

# åˆ›å»º LLM å®ä¾‹
llm = LLMFactory.create_llm(config)

# ç”Ÿæˆå“åº”
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]

response = llm.generate(messages)
print(response.content)
```

### å¸¦é‡è¯•çš„ä½¿ç”¨

```python
# é…ç½®ä¼šè‡ªåŠ¨å¤„ç†é‡è¯•
config = LLMConfig(
    model_name="gpt-4o",
    max_retries=5,  # æœ€å¤šé‡è¯• 5 æ¬¡
    retry_delay=2.0  # åˆå§‹å»¶è¿Ÿ 2 ç§’
)

llm = LLMFactory.create_llm(config)
# å¦‚æœé‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œä¼šè‡ªåŠ¨é‡è¯•
response = llm.generate(messages)
```

### æµå¼ç”Ÿæˆ

```python
# æµå¼ç”Ÿæˆå“åº”
for chunk in llm.stream_generate(messages):
    print(chunk, end='', flush=True)
```

### Token è®¡æ•°

```python
text = "Hello, world!"
token_count = llm.count_tokens(text)
print(f"Token count: {token_count}")
```

### æˆæœ¬ä¼°ç®—

```python
from src.llm.utils import estimate_cost

tokens = 1000
cost = estimate_cost(tokens, "gpt-4o")
print(f"Estimated cost: ${cost:.6f}")
```

## ğŸ”§ é…ç½®é€‰é¡¹è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model_name` | str | å¿…éœ€ | æ¨¡å‹åç§°ï¼ˆå¦‚ "gpt-4o", "o3-mini"ï¼‰ |
| `temperature` | float | 0.7 | é‡‡æ ·æ¸©åº¦ï¼ˆ0.0-2.0ï¼‰ |
| `max_tokens` | int | 2048 | æœ€å¤§ç”Ÿæˆ token æ•° |
| `top_p` | float | 1.0 | æ ¸é‡‡æ ·å‚æ•° |
| `frequency_penalty` | float | 0.0 | é¢‘ç‡æƒ©ç½š |
| `presence_penalty` | float | 0.0 | å­˜åœ¨æƒ©ç½š |
| `max_retries` | int | 3 | æœ€å¤§é‡è¯•æ¬¡æ•° |
| `retry_delay` | float | 1.0 | é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰ |
| `timeout` | int | 60 | API è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |

## ğŸš¨ é”™è¯¯å¤„ç†

### è‡ªåŠ¨é‡è¯•çš„é”™è¯¯ç±»å‹

1. **RateLimitError**: API é€Ÿç‡é™åˆ¶
   - è‡ªåŠ¨é‡è¯•ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
   
2. **APIConnectionError**: ç½‘ç»œè¿æ¥é”™è¯¯
   - è‡ªåŠ¨é‡è¯•ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿

3. **APIError**: å…¶ä»– API é”™è¯¯
   - ä¸é‡è¯•ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸

### é”™è¯¯ç¤ºä¾‹

```python
try:
    response = llm.generate(messages)
except RateLimitError:
    print("é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
except APIConnectionError:
    print("ç½‘ç»œè¿æ¥é”™è¯¯")
except APIError as e:
    print(f"API é”™è¯¯: {e}")
```

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹

### OpenAI æ¨¡å‹
- âœ… `gpt-4o`, `gpt-4`, `gpt-4-turbo`
- âœ… `gpt-3.5-turbo`
- âœ… `o1-preview`, `o1-mini`, `o3-mini`

### æœªæ¥æ”¯æŒ
- â³ `vllm:*` - æœ¬åœ° vLLM æ¨¡å‹
- â³ `ollama:*` - Ollama æœ¬åœ°æ¨¡å‹

## ğŸ” è°ƒè¯•æŠ€å·§

### å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### æŸ¥çœ‹æ¶ˆæ¯æ ¼å¼

```python
from src.llm.utils import format_messages_for_logging

formatted = format_messages_for_logging(messages)
print(formatted)
```

### éªŒè¯é…ç½®

```python
from src.llm.utils import validate_config

if validate_config(config):
    print("é…ç½®æœ‰æ•ˆ")
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **åˆç†è®¾ç½® max_tokens**: ä¸è¦è®¾ç½®è¿‡å¤§ï¼Œé¿å…æµªè´¹
2. **ä½¿ç”¨ç¼“å­˜**: å¯¹äºé‡å¤çš„è¯·æ±‚ï¼Œè€ƒè™‘å®ç°ç¼“å­˜æœºåˆ¶
3. **æ‰¹é‡å¤„ç†**: å¦‚æœå¯èƒ½ï¼Œæ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚
4. **ç›‘æ§æˆæœ¬**: ä½¿ç”¨ `estimate_cost()` è·Ÿè¸ª API ä½¿ç”¨æˆæœ¬

## ğŸ› å·²çŸ¥é—®é¢˜

1. **o1 ç³»åˆ—é™åˆ¶**: o1 ç³»åˆ—ä¸æ”¯æŒ `frequency_penalty` å’Œ `presence_penalty`ï¼Œå·²è‡ªåŠ¨å¤„ç†
2. **Token è®¡æ•°**: æŸäº›æ¨¡å‹å¯èƒ½æ²¡æœ‰ç²¾ç¡®çš„ token ç¼–ç ï¼Œä¼šä½¿ç”¨ä¼°ç®—å€¼

## ğŸ”® æœªæ¥æ”¹è¿›

- [ ] å®ç°è¯·æ±‚ç¼“å­˜
- [ ] æ”¯æŒæ‰¹é‡è¯·æ±‚
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§
- [ ] å®ç° vLLM å®¢æˆ·ç«¯
- [ ] å®ç° Ollama å®¢æˆ·ç«¯

